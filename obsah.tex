%=========================================================================

\chapter{Introduction}
\textit{This chapter describes the motivation leading to the presentation of this paper and how is it related to the SLAM++ project at FIT VUT. The objectives of the project and the subjects included in this document are briefly explained. The chapter ends describing the overall structure and contents of the remaining of the paper.}

\section{Motivation}
There is an immerse need to record and preserve our knowledge and perception of the world around us. Evidence of such tendency can be track thousands of years back in human existence as a cave paintings. Later we used more advanced techniques in form of written languages, sculptures, drawings, et cetera. Nowadays we have means to record our surroundings as we perceive it in 2D using cameras. However, it has proven difficult to process such information digitally. Even automated analysis of 2D information like typeset books is not a trivial problem and far from being mastered.

When it comes to 3D the problem gets much more difficult. Scanning 3D object reliably is nowadays possible in laboratory conditions, but there are hard limits like the size of the object, its structure or surface and material properties. Also the laboratory equipment used is much more expensive and physically larger compared to its 2D counterpart. This paper tries to address both of these problems by allowing user to create 3D model using multiple pictures of an object of interest from various sources. Such 3D model, even though it may be inaccurate, has number of applications. It can be used by archaeologists to preserve cultural heritage, by architects for spatial planning, by entertainers to create 3D models and virtual reality or by engineers to replicate existing 3D objects.

\section{Tools}
\label{sec:tools}
The process of creating a 3D model usually consist of two parts: scanning the object and reconstruction of the model. There are three main approaches how to scan physical object: contact, active non-contact  and passive non-contact scanners. The contact 3D scanners probe the subject through physical touch. The active non-contact scanners use a light in forms of laser or X-ray to scan the object while the passive non-contact scanners are using either multiple cameras, images with varying lighting conditions or silhouettes extruded from image with contrasted background. In this thesis we will be particularly interested in scanning objects using multiple images from various cameras. The input images will be downloaded from the internet, especially the Flickr services which provides great means of finding relevant photos using tags. The pipeline of transformation from 2D image to the 3D model consists of 5 steps: 1) keypoints detection 2) feature extraction  3) feature matching 4) camera initialization and pose estimation and finally 5) structure computation. In principle, the algorithm firstly finds points of interests in every image and tries to match them. Once matched the camera position can be estimated and the structure implemented as a series of 3D points called point cloud. This part will use the SLAM++ framework \cite{www:slam} as it implements many of the algorithms used in Bundle Adjustment like Nonlinear Least Square solver. Because the process is quite difficult, the resulting 3D structure contains a lot of noise and has to be filtered and segmented before a polygonal model can be created. There is number of tools available for either manual processing of point cloud data, like MeshLab \cite{www:meshlab}, or the whole process can be automatized using framework like the PCL \cite{www:pcl}. 

\section{Objectives}
This thesis aims to identify challenges and propose solution of 3D reconstruction from a set of 2D images taken by various cameras leading to creation of a software that can do so automatically. The objective is to reduce the correspondence problem between each two images and the study of camera modelling and calibration. An accurate estimation of the camera model and correspondence allows us to compute three-dimensional information from a two-dimensional image sequence. In order to eliminate artefacts the input set of images, obtained from internet, will have to be filtered to contain only daytime images from summer season.

The study of the geometry involved in multiple camera vision systems should allow us to present an application that can from a set of two-dimensional images reconstruct 3D scene depicted by the images.

\section{Overall Structure}
The thesis consist of 4 chapters and a bibliography at the end of the document.

Chapter \ref{chapter:the-state-of-the-art} introduces reader to the current state of the art algorithms used in the process of estimation three-dimensional structure from two-dimensional image sequences. Firstly, it discusses existing bundle adjustment solutions, like VisualSFM, Photosynth and Bundler, and elaborates on the output of these programs. Later they will be used as a benchmark for the implemented application. The programs mentions, will be used for the final application performance and effectiveness evaluation. Lastly the chapter focuses on the state-of-the-art features detectors, extractors and matchers, built in the SLAM++, and aims to compare theirs efficiency and performance. The comparison is then discussed and best combination in terms of performance and effectiveness chosen for the planned application.

In the chapter \ref{chapter:methodology} the whole 3D reconstruction process is thoroughly discussed and step by step explained. The steps that are already implemented, like dataset acquisition or feature detection, extraction and matching, are in detail described. The rest of the pipeline is also outlined to give the reader idea what are the next steps to be implemented. 

Finally, chapter \ref{chapter:conclusion} summarizes this document by discussing achieved goals and outlining the future work to be done within the Masters thesis.

\chapter{The State of the Art}
\label{chapter:the-state-of-the-art}
\textit{The following chapter presents to the reader existing implementation of the bundle adjustment technique. Applications described  will be used as a benchmark for the final solution once it is presented. The rest of the chapter focuses on state of the art detectors, extractors and matchers that are now accessible through the SLAM++ framework. All of them are subjected to the survey and the goal is to select the best combination in terms of performance and effectiveness for reconstruction of historic landmarks.}

\section{Existing 3D Reconstruction Applications}
\label{sec:existing_3D_reconstruction_solutions}
There are two image-based 3D reconstruction techniques based on estimating the position of the 3D points in the environment; structure from motion (SfM) and bundle adjustment (BA). The structure from motion tracks image feature from image sequences obtained by a moving camera. The problem requires camera calibration or an automatic camera parameters re-estimation. If the input image sequence consists of images from multiple uncalibrated cameras (like images from Flickr), the 3D reconstruction technique is called bundle adjustment. Both of these techniques are similar, but the bundle adjustment estimates, apart from camera poses, the calibration parameters known in SfM.

Bundle adjustment and structure from motion are very similar with simultaneous localization and mapping in robotics (SLAM). In order to deal with the inherent uncertainty, they are formulated as estimation problems and can be elegantly solved using nonlinear least squares (NLS). This is in general not an easy task, and remains a bottleneck in many large-scale robotics and computer vision applications. Without a careful approach, its complexity can scale quadratically with the size of the problem.
% Citation of the Ila's work

The problem of creating 3D reconstruction from set of images has been addressed by many research groups. In this section we will talk about few of the widely known solutions. All of the programs discussed implement a subset of the bundle adjustment pipeline briefly introduced in section \ref{sec:tools}.

\subsection*{Photosynth}
Photosynth is a software application developed by Microsoft. It is based on Photo Tourism, a research project by University of Washington graduate student Noah Snavely. Formerly the Photosynth was a 3D reconstruction software, however, in the current version output of the application is not a point cloud nor 3D model but an animation of morphing images or panorama. While it still works with images from various sources, the best result is achieved by importing photos from a single camera. Once imported, user has to choose the camera trajectory from four predefined options: spin, panorama, wall or walk. 

The Photosynth technology is using an interest point detection and matching algorithm developed by Microsoft Research which is similar in function to SIFT. Detected features are then matched between images and by analyzing subtle differences in the relationships between the features (angle, distance, etc.), the program identifies the 3D position of each feature, as well as the position and angle at which each photograph was taken. Everything is processed by Microsoft's servers and, once finished, pushed to the website or desktop/mobile application. There are little to none information about the whole process as this is a commercialized technology. \cite{www:photosynth}

\begin{figure}[ht]
	\begin{center}
		\includegraphics[keepaspectratio,width=\textwidth]{fig/Photosynth.png}
	\end{center}
	\caption{The Photosynth output of the Červená Lhota Castle in transition between several morphed images.}
	\label{fig:photosynth}
\end{figure}

\subsection*{VisualSFM}
The Chungchang Wu's Visual Structure from Motion System is a GUI application for 3D reconstruction using structure from motion (SFM). The reconstruction system is modular and integrates several of other projects: SIFT on GPU(SiftGPU), Multicore Bundle Adjustment, and Towards Linear-time Incremental Structure from Motion. VisualSFM runs fast by exploiting multicore parallelism for feature detection, feature matching, and bundle adjustment. For dense reconstruction, the program supports Yasutaka Furukawa's PMVS/CMVS tool chain, and can prepare data for Michal Jancosek's CMP-MVS. In addition, the output of VisualSFM is natively supported by Mathias Rothermel and Konrad Wenzel's SURE.

The software follows the overall 3D reconstruction pipeline; It detects features using SIFT detector and SIFT extractor, matches feature pairs with the N-View Match, estimates the camera model for each image, removes images' distortion and then runs the dense reconstruction. Outputs of feature extraction and matching are stored as a binary files and are loaded if provided to save processing time. This enables use of other than built-in extractors and matcher, at least for the sparse reconstruction. \cite{www:visual_sfm}

\begin{figure}[ht]
	\begin{center}
		\includegraphics[keepaspectratio,width=\textwidth]{fig/VisualSFM.png}
	\end{center}
	\caption{The VisualSFM application GUI with sparse reconstruction of the Červená Lhota Castle.}
	\label{fig:visualsfm}
\end{figure}

\subsection*{Bundler}
Bundler is first well known Structure from Motion (SfM) system for unordered image collections from Noah Snavely. One of the first version of this Bundler system was used in the Photo Tourism project that was aqired by Microsoft and is now part of Photosynth. 

As other applications discussed, Bundler takes a set of images, image features, and image matches as input, and produces a 3D reconstruction of camera and sparse scene geometry as output. In order to get sparse point clouds, one has to run Bundler to get camera parameters, use the Bundle2PMVS program to convert the results into PMVS2 input and then run PMVS2. The system reconstructs the scene incrementally, a few images at a time, using a modified version of the Sparse Bundle Adjustment package of Lourakis and Argyros as the underlying optimization engine. Bundler has been successfully run on many Internet photo collections, as well as more structured collections. \cite{www:bundler}

The bundler was modified and used in Photo Tourism \cite{article:photo_tourism} \cite{article:photo_tourism2} project that aims to browse large collections of photographs in 3D. The algorithm behind Photo Tourism was further modified to be used in reconstruction of entire cities. The project Rome in a Day  \cite{article:rome_in_a_day} \cite{article:reconstructing_rome} \cite{article:rome_in_a_day2} reconstructs the city of Rome from more than two million photographs.

\subsection*{SLAM\textunderscore frontend}
The SLAM\textunderscore frontend, developed at the Faculty of Information Technology at Brno University of Technology, is a framework for the bundle adjustment and structure from motion applications. The whole framework (SLAM++ and SLAM\textunderscore frontend) will be further discussed later in this thesis (section \ref{sec:slam}), but right now we want to present implemented applications that are performing the bundle adjustment, respectively structure from motion, algorithms. Right now there are three finished applications:

\begin{itemize}
	\item The \textbf{Mono app} which reconstructs 3D scene from a video made by single camera which camera calibration is known.
	
	\item The \textbf{Spheron app} reconstruct the 3D scene from a spheron camera with known calibration.
	
	\item Lastly, the \textbf{Stereo app} uses a stereo vision to reconstruct the 3D scene from a pair of horizontally displaced calibrated cameras.
\end{itemize}

Apart from the applications listed above, two other are under development: the Activity app and the Uncalibrated app which is the subject of this thesis and will be implemented in the future.

\section{Detectors}
\label{sec:detectors}
A successful 3D reconstruction stands and falls on good features detection.The quality and robustness of features is (usually) much more important then their quantity which will be demonstrated by the Dense detector later in this section. The ideal feature detector finds salient image regions such that they are repeatedly detected despite change of viewpoint; more generally it is robust to all possible image transformations. Therefore, it does not detect any points in uniform and uninteresting surfaces like sky or texture-less walls. The best detector to be used depends heavily on the requested task. In our application features we are interested in are edges and corners of buildings and their distinct parts.

We can divide types of image features into following categories (please note that a detector can detect features from multiple categories):
% CITATION NEEDED!!!
\begin{itemize}
	\item \textbf{Edge} is a point where there is a sudden change between adjacent pixels (strong gradient magnitude). Generally an edge can be of almost any arbitrary shape and may include junctions. Locally edges have a one-dimensional structure.
	\item \textbf{Interest point} has a local two dimensional structure. We can think of it as two-dimensional edge, in fact early algorithms were used to detect interest points as edges and then selected the interest points by further calculation. In some literature you the interest points may be referred to as corners.
	\item \textbf{Blobs} provide a complementary description of image structures in terms of regions, as opposed to corners that are more point-like. A term regions of interest or interest points are sometimes used as the blob descriptors often contain a preferred point (a local maximum or a center of gravity). Blobs allows detection of smooth areas in an image that might not be detected as an edge or corner.
	\item \textbf{Ridges} are in computer vision a set of curves whose points are have a local maximum in at least one dimension. This notion captures the intuition of geographical ridges. Ridge detection is usually much harder then Edge, Interest point or Blob detection.
\end{itemize}

The detector algorithms used are implemented in the Open source Computer Vision (OpenCV) \cite{www:opencv} library which is used by the SLAM++. The OpenCV was designed for computational efficiency with a strong focus on real-time applications. It contains number of algorithms from computer vision and is being recognised. In the remainder of this section feature detectors wrapped in the SLAM++ will be presented. Each detector will be run on the same set of images with historic landmarks in order to evaluate the effectiveness and speed. Then a summary of the results will be presented.
 
\begin{itemize}
	\item A \textbf{Scale Invariant Feature Transform (SIFT)} keypoint is image region with an orientation. The detector uses as a keypoints image structures  which resemble blobs. The use of the detector is licenced which is one of the reasons why we would like to use a different detecter. However, as expected, the detector performs very well and is used by other SfM software we discussed earlier. \cite{article:sift}
	
	\item The \textbf{Speeded Up Robust Features (SURF)} detector is modification of the SIFT detector. It addresses slow processing of the SIFT while maintaining reasonable efficiency. While it can surely be used in the SfM application, from our experiments we discovered that the increased performance greatly decreases feature detection for (in our case) important structures.  \cite{www:surf}
	
	\item The \textbf{Features from Accelerated Segment Test (FAST)} aims to rapidly increase performance of feature detection while sustaining feature quality of SIFT-like detectors. The algorithm detects corners in the image and should be used with SIFT or SURF extractor for best performance. As the FAST select in our case three times more features hundred times faster than SIFT (resp. 50 times faster than SURF) we market this as one of the interesting detectors for the final implementation. \cite{article:fast}
	
	
	\item The \textbf{Robust Invariant Scalable Keypoints (BRISK)} detector uses scale-space pyramid layers of octaves and intra-octaves to detect corners in an image. The algorithm uses FAST feature detector score and was developed to get the better of SIFT and SURF detectors. However, in our case the performance gain is not worth decreased feature quality. \cite{article:brisk}
	
	\item \textbf{Dense Sampling} uses a regular grid to find a keypoints in the image. This results in good coverage of the entire object or scene and a constant amount of feature per image area. The dense sampling is fast as the detector selects all points on a grid without analysis of the surrounding. On the downside, dense sampling cannot reach the same level of repeatability as obtained with interest points, unless sampling is performed extremely densely, but then the number of features quickly grows unacceptably large. The dense sampling is therefore not useful in the SfM model estimation, but can be used for a dense reconstruction once sparse structure is calculated. \cite{article:dense}
	
	\item One of the most known feature detectors is the \textbf{Harris Corner Detector} . It can identify similar regions between images that are related through affine transformations and have different illuminations. Even though the Harris Corner Detector is fast, it does not select enough keypoints and therefore is not suitable for the 3D building reconstruction. \cite{www:harris}
	
	\item The \textbf{Good Features to Track (GFTT)} detector is modified version of the Harris Corner Detector described earlier. It is still classified as a corner detector, however, the scoring function differs. Compared to the Harris, the algorithm was slightly slower, with higher amount of features. Nevertheless, both of these algorithms do not perform well enough for our problem. \cite{article:gftt}
	
	\item The \textbf{Oriented FAST and Rotated BRIEF (ORB)} detector originated from the OpenCV Labs. Its goal was to offer robustness of a SIFT and SURF, while maintaining fast processing time like FAST and BRIEF combination. While this may be true, for our problem the ORB detector does not perform well enough. The feature found rarely belong to a building and usually chunks around trees and vegetation. \cite{www:orb}\cite{article:orb}
	
	\item The \textbf{Maximally Stable Extremal Regions} is a blob detector. For our task this detector performs poorly and takes even more time than SIFT detector.
\end{itemize}

\section{Extractors}
\label{sec:extractors}
In order to work further with the keypoints detected in previous step, the keypoints have to be analysed and transformed into so called feature descriptor. The process consist of inspecting local image patch around the keypoint to be extracted. This extraction may involve quite considerable amounts of image processing and involves reducing the amount of resources required to describe the original data. The result is known as a feature descriptor or a feature vector. Among the information that may be stored within feature descriptor, one can mention local histograms. In addition to such attribute information, the keypoints detection step may also provide complementary attributes, such as the edge orientation, gradient magnitude in edge detection and the polarity or the strength of the blob in blob detection. The authors of detectors usually specify which extractor should work best for their detection algorithm, some even provide their own. Nevertheless, we tried all combinations with detectors to get the best results for our application. However, note that all extractors were used with the default parameters. Final results when, for our problem, better parameters are set may differ.

There are two types of descriptors in the OpenCV that are now available in Slam++ frontend; a) descriptors using floating point and b) descriptors storing information as a binary data in unsigned char type.
\begin{itemize}
	\item[a)] \textbf{Float} descriptors:
	
	\begin{itemize}
		\item \textbf{SIFT:} The scale-invariant feature transform of a neighbourhood is a 128-dimensional vector of histograms of image gradients. The region, at the appropriate scale and orientation, is divided into a $4\times 4$ square grid, each cell of which yields a histogram with 8 orientation bins. The SIFT extractor is advised to be used with the SIFT, SURF and FAST detector.
		\item \textbf{SURF:} The speeded up robust feature extractor uses either 128 or 64-dimensional vector of histograms of image gradients.An oriented quadratic grid of $4 \times 4$ square sub-regions is laid over the keypoint and a wavelet response computed for each square. According to literature the SIFT, SURF and FAST detector can be used with the SURF extractor. \cite{www:sift_surf}
	\end{itemize}
	
	\item[b)] \textbf{Binary} descriptors:
	
	\begin{itemize}
		\item \textbf{BRIEF:} The Binary Robust Independent Elementary Feature descriptor is a 128, 256 or 512-dimensional bitstring which is a good compromise between speed, storage efficiency and recognition rate. The descriptor is much smaller (16, 32 or 64 bytes) compared to floating point descriptors, while maintaining a good performance compared to SURF or U-SURF. \cite{article:brief}
		
		\item \textbf{ORB:} Unlike BRIEF, Oriented FAST and Rotated BRIEF (ORB) is comparatively scale and rotation invariant while still employing the very efficient Hamming distance metric for matching. As such, it is preferred for real-time applications, but may be suitable for some offline applications as well. \cite{www:orb}\cite{article:orb}
		
		\item \textbf{FREAK:} The Fast Retina Keypoint extractor aims to be faster and more robust than SIFT and SURF. It uses a novel keypoint descriptor inspired by the human visual system to compute cascade of binary strings.\cite{article:freak}
		
		\item \textbf{BRISK:}  The  Binary Robust Invariant Scalable Keypoints extractor uses a 64-byte binary descriptor composed as a binary string by concatenating the results of simple brightness comparison tests. \cite{article:brisk}
	\end{itemize}
\end{itemize}

\section{Matchers}
\label{sec:matchers}
So far we are able to find points of interest in an image and describe them in such a way that they are effectively stored but still contain information about the point and its local image patch. Once descriptors are extracted from two or more images, we want to match points present more then one image. This is called nearest neighbour search which is an optimization problem for finding closest (or most similar) points. There are two approaches to this problem that are implemented in the OpenCV: Brute-Force and FLANN-based matching.

The Brute-Force matcher is simple and naive approach. It takes the descriptor of one feature from the first image set and matches it with all other feature from the second image set. During the process a distance of some sort is calculated and the match with best metric selected. There are number of metrics implemented in the OpenCV to be used with different descriptors but we, once again, tried all the combinations in order to get best result for our problem. The algorithm promises best possible matches, but due to the fact that it tries to match each pair of feature, can take a lot of time to process. 

The Fast Library for Approximate Nearest Neighbors (FLANN) implemented in OpenCV, performs a fast approximate nearest neighbour searches in high dimensional spaces. It uses the Hierachical K-means Tree for generic feature matching. Nearest neighbors are discovered by choosing to examine the branch-not-taken nodes along the way. \cite{www:flann}

\chapter{Methodology}
\label{chapter:methodology}
\textit{This chapter thoroughly describes the so far implemented part of the process of 3D reconstruction from a set of images. Steps that are not yet implemented are outlined and briefly discussed. The reader will be introduced to the dataset aquisition from the internet especially Flickr. Then we will briefly talk about the implemented feature detection, extraction and matching in context of 3D reconstruction. The next section discusses the 3D reconstruction approaches leading to the uncalibrated one which will be implemented. Finally we introduce the SLAM++ framework and it is application in this project.}

\section{Three-dimensional Structure Estimation Pipeline}
The pipeline (depicted in figure \ref{fig:pipeline}) for the to be implemented application consists of 7 distinct steps:
\begin{itemize}
	\item[1)] \textbf{Dataset aquisition.} First step in the bundle adjustment pipeline is selection of a input data. Specific requirements on the data varies throughout different software, however, we can generalize some properties of such set of images. The set has to contain images that are overlapping one another. Only such images are used in the reconstruction as they provide points seen by multiple cameras and therefore the 3D position can be calculated.
	\item[2)] \textbf{Keypoints detection.} Keypoints are parts of the image that are significant in some way. The significance is usually caused by a sudden change in gradient on relatively small part of the image. These points will be used to estimate the 3D representation. We will focus more on keypoints detection in section \ref{sec:detectors}. 
	\item[3)] \textbf{Feature extraction.} The detected keypoints are rarely used as provided by the detector as they do not provide enough information about the point itself. A set of calculations is applied in order to extract data from the surroundings of such point and enrich information about the keypoint. Resulting structure is called feature and contains all data required for next step. At this point the input image does not have to be kept in memory anymore.
	\item[4)] \textbf{Feature matching.} Now that we have keypoints represented as feature we want to establish a visual correspondence between a set of keypoints from two closely related images. This is done by so called feature matching and was discussed in detail in section \ref{sec:matchers}. Up to this day, feature matching is the last step we've managed to implement in our application.
	\item[5)] \textbf{Camera initialization and pose estimation.} Once we know correspondence between two closely related images, we can estimate the parameters of the camera(s). If not known, the camera matrix can be calculated and the relation between the points in 3D.
	\item[6)] \textbf{Structure computation.} Next step is to calculate the model's 3D structure from the points seen by different cameras. The quality of the structure is greatly affected by errors from previous steps.
	\item[7)] \textbf{Visualization.} Lastly the resulting 3D structure in form of point cloud is visualized.The visualization can be either just a set of images cleverly arranged to give impression of 3D, cloud of 3D points or polygonal model. In this thesis we will not be interested in the visualization of the resulting model, however we see this as an important part of the bundle adjustment pipeline. As of now there are no plans to implement visualization of a final sparse reconstruction, but the SLAM++ fremework provides a means to visualize the resulting sparse point cloud. 
\end{itemize}

\begin{figure}[ht]
	\begin{center}
		\includegraphics[keepaspectratio,width=\textwidth]{fig/pipeline.pdf}
	\end{center}
	\caption{The three-dimensional structure from two-dimensional images estimation pipeline to be implemented in the final application.}
	\label{fig:pipeline}
\end{figure}

\subsection*{Dataset Generation}
Creating a dataset is a crucial part of the process of estimating three-dimensional structures from two-dimensional image sequences. The dataset has to contain enough images with a feature pairs to be vialable for reconstruction. We also want to filter out images taken during the night or throughout various seasons as the depicted object and its surroundings may change significantly.

With this in mind we've decided to download images from Flickr webpage. Flickr is well known and widely used web service for sharing pictures. The advantage of this service, in comparison to other picture sharing websites, is the ability to tag the photo. Each image can contain a number of tags describing it. Most of the pictures uploaded contain information about the place where the photo was taken and can be aggregated by that tag. A tool, Flickr downloader, we designed allows downloading images with specified tag from Flickr in batches. It is an easy to use Python script expecting two parameters; number of photos to be downloaded and a tag to be browsed. The script connects to the Flickr webpage, downloads the search results page, cyclically opens each photo page and downloads the image. The downloaded images are stored to the \texttt{downloaded} output folder.

While the Flickr yelds good results for well known places in countries like USA or western Europe, there is not enough images in the Czech Republic yet. This is the reason why we have, for now, decided to use different services, like Google image search, as well. The datasets aquired were manually filtered to eliminate irrelevant images and limit the selection to daytime photos taken in summer.

\subsection*{Feature Detection, Extraction and Matching}
Once we have a desired dataset we compute the feature pairs. An interface between the SLAM++ and the OpenCV was created which allowed use of the feature detectors, extractors and matchers described in chapter \ref{chapter:the-state-of-the-art}. The interface is a C++ template so there is no overhead. The extractors are defined using the \texttt{typedef} keyword and naming convention follows this scheme: \texttt{FeatureExtractor\_OpenCV\_[detector]\_[extractor]}.

In the building reconstruction the repeated features are common. Many objects, such as clock towers with nearly identical sides, or domes with strong radial symmetries, pose challenges for structure from motion. When similar but distinct features are mistakenly equated, the resulting 3D reconstructions can have errors ranging from phantom walls and superimposed structures to a complete failure to reconstruct. This can be partially solved by a good selection of the feature detecting, extracting and matching algorithms. In section \ref{sec:matchers} we have identified some combinations that yeld good feature matches and therefore are suitable for the Structure from Motion algorithms on building reconstruction. We will use these algorithms in the uncalibrated app once it is finished.

\section{3D Reconstruction Approaches}
Based on the image source we can distinguish between three types of vision: Stereoscopic, Monocular and Uncalibrated. The type of the vision defines the difficulty of the 3D reconstruction problem. \cite{book:multiple_view_geometry}

\subsection*{Stereoscopic Vision}
The stereoscopic vision is similar to human binocular vision. Two cameras, displaced horizontally from one another are used to obtain view of the scene, simulating human vision. By comparing both images, the relative depth information can be obtained, which is proportional to the differences in distance to the objects. If the images are undistorted, and camera parameters known, we can easily calculate the 3D relative position of the points. Figure \ref{fig:stereo} shows how the depth can be calculated using following equation:

\begin{equation}
	z=\frac{f b}{x_L - x_R}
\end{equation}

If the cameras are calibrated then the reconstruction is straightforward as the depth is the only thing required to calculate the position of the point in 3D.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[keepaspectratio,width=9cm]{fig/stereo.pdf}
	\end{center}
	\caption{An illustration of obtaining the depth information from stereo image.}
	\label{fig:stereo}
\end{figure}

\subsection*{Monocular Vision}
The monocular vision consists of one camera moving in space. Because we have only one camera, we can't easily calculate the depth as in stereoscopic vision, but we need to estimate the camera position first. Firstly we detect feature in images and match them. Now that we have paired points in images, we can begin to estimate the camera position. At least three image pairs are needed in order to calculate the camera pose, other points can increase the accuracy of the solution. Once camera position is known, the 3D structure can be calculated. However, because the picture is distorted and pixels have limited precision the structure will introduce some error. In order to minimize the reprojection error, we run a bundle adjustment algorithm.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[keepaspectratio,width=\textwidth]{fig/mono.pdf}
	\end{center}
	\caption{An illustration of the process of 3D reconstruction using a monocular vision. a) detect features b) match them and estimate camera pose c) add other points with next iteration. This image was taken from the Monocular camera 3D reconstruction presentation by Ing. Marek Šolony.}
	\label{fig:mono}
\end{figure}

\subsection*{Uncalibrated Vision}
Until this point the camera parameters were known or could have been easily estimated. However, when dealing with images downloaded from the internet, we don't have such information. This makes the problem much more complex and to find the solution we not only need to estimate camera position but also its parameters. The principle of reconstruction is similar as in the monocular vision, but we need 8 points instead of 3 to calculate the two-dimension to three-dimension transformation. Once the camera calibration and position is known, we can undistort the images and calculate the 3D structure in similar fashion as in the monocular.

\section{SLAM++ and SLAM\textunderscore frontend}
\label{sec:slam}
The SLAM++ is a library containing several methods used in problems like bundle adjustment (BA), structure from motion (SfM), simultaneous localization and mapping (SLAM) and many others. The library includes general graph optimizer along with several problem solvers, nonlinear least squares solvers and block linear solvers.  It is written in C++ and it is very fast due to the fact that it exploits the block structure the problems and offers very fast solutions to manipulate block matrices within iterative nonlinear solvers.

SLAM\textunderscore frontend \cite{www:slam_frontend} is a collection of applications that take as input image files or any other sensor data files and generate inputs for SLAM++ block-sparse linear algebra SLAM solver. Apart form SLAM++, it requires OpenCV. The SLAM\textunderscore frontend is also an easy to use interface for implementing custom structure from motion or bundle adjustment application. It provides means to load set of images and detect, extract and match feature. The framework provides templates for custom estimators, such that one can easily implement camera pose estimator for a BA application.

\chapter{Math}
camera definition (intrinsic, extrinsic)
\begin{verbatim}
http://dsp.stackexchange.com/questions/2736/step-by-step-camera-pose-estimation-for-visual-tracking-and-planar-markers
http://docs.opencv.org/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#solvepnpransac
https://en.wikipedia.org/wiki/Camera_resectioning
\end{verbatim}
, projection, homography, fundamental, essential matrix...

\chapter{Implementation}
flickr downloader, 
\section{Camera Calibration}
\label{sec:camera_calib} 
data structures, frontend etc...
\begin{verbatim}
http://www.morethantechnical.com/2012/02/07/structure-from-motion-and-3d-reconstruction-on-the-easy-in-opencv-2-3-w-code/
https://github.com/royshil/SfM-Toy-Library
http://www.morethantechnical.com/2012/08/09/decomposing-the-essential-matrix-using-horn-and-eigen-wcode/
https://code.google.com/p/libmv/feeds
https://code.google.com/p/libmv/wiki/Roadmap#Version_3.x

http://nuxsfm.sourceforge.net/wiki/index.php/Computer-vision
\end{verbatim}

\chapter{Experiments Evaluation}
\textit{This chapter presents the experimental results obtained on an artificial and real scenes by means of the algorithms described in previous chapters. In the beginning of the chapter we will present the datasets used for the evaluation and describe their characteristics. These datasets will be used to determine precision of the program. Also this chapter presents qualitative results obtained from our program as well as other existing solutions and their evaluation and comparison to reference values where available. Lastly we will compare our program to other previously mentioned programs in terms of complexity and resources consumption. }
\section{Introduction}
\label{sec:experiments-introduction}
This chapter presents the experimental results obtained from the implementation of the system described in previous chapters. After each experiment a brief discussion of a results is included with the aim of giving the reader further details. First, we introduce the datasets that are being tested. Then we present evaluation of the feature detection, extraction and matching methods with the aim of selecting the best combinations for our problem. The section \ref{sec:experiments-calibrated} presents results obtained from the program when the intrinsic camera parameters are known on both ordered and unordered image sets. It also compares each implemented camera tracking algorithm and elaborates on which one is best in which scenario. In the last section \ref{sec:experiments-weakly} reader finds qualitative results for the most general case: weakly calibrated bundle adjustment.  In order to provide as precise results as possible, if not stated otherwise, all programs were compiled and run separately, without any user intervention on a machine with following specification:
\vspace{.5cm}

\begin{tabular}{ l  l }
	Model: & MacBookPro6,1 (2010) \\
	OS: & OS X 10.10.4 \\
	CPU: & Intel Core i5 2.53 GHz \\
	RAM: & 8 GB 1067 MHz DDR3 \\
	GPU: & NVIDIA GeForce GT 330M and Intel HD Graphics  \\
	HDD: & OCZ-AGILITY4 512 GB \\
	Compiler: & Apple LLVM version 6.1.0 (clang-602.0.53) \\
\end{tabular}

\vspace{.5cm}
To evaluate output camera positions as well as structure we are using a cross-platform open source 3D animation suite Blender \cite{www:blender}. The main reason for this choice was, apart from native support for PLY file format, easy extensibility with custom python scripts. Because the output of all SfM and BA programs is at least ambiguous to scale and rotation, the Precise Align extension \cite{www:blender_precise_align} is used to match cameras and structure to the 3D model. A detail on how exactly is the resulting structure matched to the 3D model is specific for each case and usually depends on landmarks in the scene. The camera position error is calculated as a root-mean-square error against reference values using formula:
\begin{equation}
	RMSE=\sqrt{\frac{\overset{n}{\underset{k=0}{\sum}} (x_{k_2}-x_{k_1})^2 + (y_{k_2}-y_{k_1})^2 + (z_{k_2}-z_{k_1})^2}{n}},
	\label{eq:RMSE}
\end{equation}
where $n$ is a number of cameras and $x_{k}, y_k, z_k$ are the 3D camera's coordinates. This value is normalized as we match positions of the first and last camera with reference cameras and the middle camera to be as close as possible to the middle reference camera.

\section{Datasets}
In order to test our programme we have collected a number of datasets. These datasets usually offer not only images, but also intrinsic and extrinsic camera parameters and a 3D model or 3D points and lines. You may find all of these datasets along with experiment's results on the attached DVD.

\subsection*{Model House}
The University of Oxford provides a number of  datasets used in many other papers for a benchmark \cite{www:oxford_models}. Some of these datasets contain images, camera parameters, 2D geometry (interest points, line segments and matches) and 3D geometry (points, line segments and camera matrices). However, these datasets usually lack the intrinsic camera parameters apart from the Model House which we will be using. This dataset is a sequence of ten black and white images rotating 90 degrees around a model of house in a clockwise direction without any distortion and with the principal point in the centre of the images. The model also contains the reference positions of the cameras as well as 3D points and 3D lines. Out of the 3D points and lines we have created a simple 3D model that is used for visual comparison of the structure, but as this is not a reference model no qualitative results will be given.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[keepaspectratio,width=\textwidth]{fig/experiments-model-house.pdf}
	\end{center}
	\caption{The Model House dataset showing first, middle and last images of the sequence.}
	\label{fig:modelhouse}
\end{figure}

\subsection*{Temple of the Dioskouroi}
The Temple of the Siodkouroi in Agrigento (Sicily) is a dataset packed in the \textbf{libmv} library \cite{www:libmv}. It is an ordered sequence of 15 coulored images creating a full 360 degree rotation around a 3D model of temple. The dataset contains camera calibration parameters, the 3D positions of the cameras and a tight rectangular bounding box of the temple model. 

\begin{figure}[ht]
	\begin{center}
		\includegraphics[keepaspectratio,width=\textwidth]{fig/experiments-temple.pdf}
	\end{center}
	\caption{The Temple of the Dioskouroi dataset showing first five images of the sequence.}
	\label{fig:temple}
\end{figure}

\subsection*{The City of Sights: An Augmented Reality Stage Set}
The City of Sights \cite{cityofsights_ISMAR2010} is a complex dataset by Graz University of Technology, Four Eyes Lab, University of California at Santa Barbara and Muncich University of Technology. This dataset was specifically designed for a variety of Augmented Reality research. The images in this datasets were captured by a robotic arm with calibrated camera. The whole scene can be downloaded as a 3D model along with the camera tracks (which were taken from a paper model of the scene). The camera movement between frames is quite small (around 0.1 mm) therefore we will be using every fifth or tenth frame. We have chosen the CS\_FARO\_12 and CS\_ART\_12 datasets which comprise of images depicting the centre of scene.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[keepaspectratio,width=\textwidth]{fig/experiments-city-of-sights.pdf}
	\end{center}
	\caption{The City of Sights dataset with a 3D model (left) and a picture from the CS\_FARO\_12 dataset (right).}
	\label{fig:cityofsights}
\end{figure}

\subsection*{Cathedral}
The cathedral dataset is a sequence of 92 images of a front face of cathedral that are not absolutely ordered. It is ensured that any pair in sequence has enough keypoints matches, but the camera trajectory does not always move in one direction. The intrinsic camera parameters are known with a principal point at the centre of each image and the images have no distortion. A rough 3D model is known but no reference camera poses are offered.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[keepaspectratio,width=\textwidth]{fig/experiments-cathedral.pdf}
	\end{center}
	\caption{The cathedral dataset with a rough 3D model (left) and a picture from the dataset (right).}
	\label{fig:cathedral}
\end{figure}

\subsection*{Slezské divadlo, Opava}
This dataset is a sequence of images taken by us using an iPhone with estimated camera calibration using OpenCV sample described in section \ref{sec:camera_calib}. The aim of this dataset is to demonstrate that our program (and associated utilities) offers complete solution that can reconstruct 3D model from any camera.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[keepaspectratio,width=\textwidth]{fig/experiments-divadlo.pdf}
	\end{center}
	\caption{The Slezské divadlo in Opava dataset obtained using camera with estimated calibration.}
	\label{fig:divadlo}
\end{figure}

\subsection*{Zámek Červená Lhota}
The Zámek  Červená Lhota is collection of 235 pictures from Flickr, Google Images and other websites. Therefore the images have various camera calibration and distortion. However, we have inspected each image manually to ensure that neither is flipped horizontally nor vertically. This dataset is used to evaluate general bundle adjustment.

\begin{figure}[ht]
	\begin{center}
		\includegraphics[keepaspectratio,width=\textwidth]{fig/experiments-cervena-lhota.pdf}
	\end{center}
	\caption{Sample of the Zámek Červená Lhota dataset which contains images from various sources.}
	\label{fig:cervena-lhota}
\end{figure}

\section{Feature Detectors, Extractors and Matchers}
\label{sec:experiments-extractors}
One of the key components for the SfM and BA application is selection of the keypoints in the input images and their matching for the specific application. We have conducted a number of experiments in order to evaluate the detectors, extractors and matchers available in OpenCV and SLAM\textunderscore frontend. The main goal is to select the best combination that detects the most relevant keypoints in pictures of buildings in a reasonable time. Figure \ref{fig:detectors} shows three detectors (SIFT, SURF and FAST) that are suitable for our task as they find enough relevant features in an image. 

\begin{figure}[!htbp]
	\begin{center}
		\includegraphics[keepaspectratio,width=\textwidth]{fig/detectors.pdf}
	\end{center}
	\caption{Results of the feature detection evaluation on as set of 250 various images from the Červená Lhota dataset. Graph a) shows average time necessary for processing an image using selected detector. In graph b) you can find how many features on average were detected in a single image.}
	\label{fig:detectors}
\end{figure}

\begin{figure}[!htbp]
	\begin{center}
		\includegraphics[keepaspectratio,width=\textwidth]{fig/matchers.pdf}
	\end{center}
	\caption{Results of the feature detection, extraction and matching evaluation on a set of 100 image pairs from the Červená Lhota dataset. The interesting combinations are labeled. Note that combination taking more than 120 seconds to compute were omitted.}
	\label{fig:matchers}
\end{figure}

Then we have manually selected 100 image pairs from the Červená Lhota dataset and tried every feature detection, extraction and matching combination available. The results are shown in figure \ref{fig:matchers}. To select only potentially good matches ($G$), following metric was applied

\begin{equation}
	0.02 \leq G \leq 2* M
\end{equation}

where $M$ is a minimal distance found between a match pair for selected images. From the results, we can conclude that best result, in terms of performance to effectiveness ratio, is achieved using FAST detector, SIFT extractor and FLANN matcher. Figure \ref{fig:matches} shows matches for one image pair using some of the well known feature detector, extractor and matcher combinations. The picture a) shows that the ORB detector is fast, but for our application does not yeld good results. The SIFT detector and extractor performs well and can be used in the SfM and BA applications. In fact it is being used by nearly ever other program (VisualSfM, Bundler, OpenMVG). The best obtained result is depicted in c) where FAST detector and SIFT extractor were used.

\begin{figure}[!htbp]
	\begin{center}
		\includegraphics[keepaspectratio,width=\textwidth]{fig/matches.pdf}
	\end{center}
	\caption{Examples of various feature detector, extractor and matchers combinations on an image pair. There are 307 good matches in picture a) and the whole process took 0,487 seconds. The picture b) took 12.7 seconds to process and has 274 good matches. The last picture c) contains 1804 good matches and was processed in 23.853 seconds.}
	\label{fig:matches}
\end{figure}


\section{Calibrated Case}
\label{sec:experiments-calibrated}
Once we have estimated which feature detectors, extractors and matchers are suitable for our application, we can continue evaluating the performance of the pose estimation and structure estimation. There are two cases to evaluate: when camera calibration is known and when it is not available and needs to be estimated.  Another distinction is whether the input sequence image is ordered or not. Let us start with the easier case; known camera calibration and ordered sequential image input.

\subsection*{Ordered Case}
\label{sub:ordered_case}
Because the input is ordered, there is no need to calculate matches between every image pair, but we can build a single camera track containing all the pictures in a sequence. The suitable datasets for this case are the Model House and Temple of Dioskouroi. The first thing to evaluate is the memory and time complexity. The program was compiled and run with 10 times for each case using three different detectors; FAST, SIFT and SURF with both both cached and uncached option. On top of that each scenario was run with and without the optimizer. 

The figure \ref{fig:rss_memory} shows how does the RSS memory changes throughout the run. For reader's easier orientation we have marked distinct phases of the program. One can observe how demanding, in terms of memory, is feature detection and extraction. After the keypoints are matched, the program can not immediately free all feature as it needs them to calculate fundamental matrix between the first image pair. Once done the feature can be unallocated and camera pose estimation and structure reconstruction process starts with next pair that only needs 3D - 2D correspondences.

\begin{figure}[!htbp]
	\begin{center}
		\includegraphics[keepaspectratio,width=\textwidth]{fig/exp_model_house_memory.pdf}
	\end{center}
	\caption{An example of the program run showing RSS memory on the Model House dataset using uncached SIFT extractor and detector.}
	\label{fig:rss_memory}
\end{figure} 

The table \ref{tab:time_complexity} shows what is the RSS memory and computation time of various combinations of the program. What is rather strange is, that if cached SIFT detector and extractor are used the detection and extraction time lowers significantly, but the reconstruction time increases marginally. The fact that this affects only the SIFT detector makes it rather strange. This is probably related to memory access as the output for both cached and uncached options are the same. Also note that the time when the structure is refined by the SLAM++ optimizer is significantly smaller than when it's not, yet the result is much better (as described later in this section). Unfortunately there is no base of comparing our application with other solutions as the VisualSFM uses GPU algorithms and the OpenMVG fails with uncaught exception when attempting to get intrinsic camera parameters.

\begin{table}[!htbp]
	\begin{center}
		\begin{tabular}{| l | l | l | l | l | l | l |}
			\hline
			\textbf{Detector} & \begin{tabular}[l]{@{}c@{}}\textbf{Feature}\\ \textbf{cached}\end{tabular} & \begin{tabular}[l]{@{}c@{}}\textbf{Structure}\\ \textbf{optimized}\end{tabular} & \begin{tabular}[l]{@{}c@{}}\textbf{Detection}\\ \textbf{time}\end{tabular} &  \begin{tabular}[l]{@{}c@{}}\textbf{Matching}\\ \textbf{time}\end{tabular}  & \begin{tabular}[l]{@{}c@{}}\textbf{Total}\\ \textbf{time}\end{tabular} & \begin{tabular}[l]{@{}c@{}}\textbf{Peak RSS}\\ \textbf{memory}\end{tabular}   \\ \hline 
			FAST & yes & yes & .1210 & .6848 & 14.1492 & \\ \hline 
			FAST & yes & no & .1092 & .6762 & 22.9874 & \\ \hline 
		 	FAST & no & yes & 7.8928 & 3.9695 & 24.2021 & \\ \hline  
		 	FAST & no & no & 7.8734 & 3.9907 & 34.6032 & \\ \hline  
		 	SIFT & yes & yes & .0571 & .1536 & 20.6526 & \\ \hline  
		 	SIFT & yes & no & .0523 & .1552 & 20.8367 & \\ \hline  
		 	SIFT & no & yes & 5.7621 & .8463 & 9.4593 & \\ \hline  
		 	SIFT & no & no & 5.8035 & .8546 & 19.7536 & \\ \hline  
		 	SURF & yes & yes & .0723 & .4800 & 8.9775 & \\ \hline  
		 	SURF & yes & no & .0671 & .4742 & 16.5755 & \\ \hline  
		 	SURF & no & yes & 25.1957 & 1.7415 & 35.4105 & \\ \hline  
		 	SURF & no & no & 25.2083 & 1.7537 & 44.7901 & \\ \hline  
		\end{tabular}
		\caption{Average time and memory complexity of the programme run on the Model House dataset. The feature are extracted using SIFT extractor and matched by FLANN matcher.}
		\label{tab:time_complexity}
	\end{center}
\end{table}
\begin{table}[!htbp]
	\begin{center}
		\begin{tabular}{| l | l | l | l |}
			\hline
			\textbf{Detector} & \textbf{Feature} &  \textbf{Matches}  & \textbf{Matches after RANSAC}  \\ \hline 
			FAST & 4727.2 & 1867.6 & 977.7 \\ \hline 
			SIFT & 1169.2 & 555 & 341.6 \\ \hline 
			SURF & 2030.5 & 848.6 & 408.2 \\ \hline 
		\end{tabular}
		\caption{Time and memory complexity of the programme on the Model House dataset.}
		\label{tab:keypoints_matches}
	\end{center}
\end{table}

To give a full insight on the problem, table \ref{tab:keypoints_matches} shows how the feature, matches before and after RANSAC count changes in respect to different feature detectors. These data directly affect number of structure points and precision of the reconstruction and pose estimation. Up until this point there was no reason for any comparison between our solution and other programs. However, we can quite easily evaluate the RMSE of the pose estimation using the Blender and equation \ref{eq:RMSE}.  The results can be found in table \ref{tab:precision}. The difference between run with and without optimizer is apparent not only from the time complexity but also when it comes to precision. While the valid structure size does not change much, both the reprojection error and camera pose RMSE is improved by factor of ten if the system is optimized after each camera addition. When compared to the Visual SFM, our program creates at about the same structure size but the RMSE is double. It is worth noting that the Visual SFM does not reconstruct sequentially but instead matches each pair of images and calculates the structure from the whole system, while our solution in sequential mode only pairs each image with two other.

\begin{table}[!htbp]
	\begin{center}
		\begin{tabular}{| l | l | l | l | l | l |}
			\hline
			\textbf{Program} & \textbf{Detector} & \begin{tabular}[l]{@{}c@{}}\textbf{Structure}\\ \textbf{optimized}\end{tabular} & \begin{tabular}[l]{@{}c@{}}\textbf{Structure}\\ \textbf{points}\end{tabular} &  \begin{tabular}[l]{@{}c@{}}\textbf{Reprojection}\\ \textbf{error}\end{tabular}  & \begin{tabular}[l]{@{}c@{}}\textbf{Camera pose}\\ \textbf{RMSE}\end{tabular}  \\ \hline 
			ours & FAST & yes & 5391 & 2403.83 & 0,04564 \\ \hline 
			ours & FAST & no & 5289  & 66365.1 &  0.45026 \\ \hline 
			ours & SIFT & yes & 1702 & 333.498 &  \\ \hline 
			ours & SIFT & no & 1326 & 36439.5 & \\ \hline 
			ours & SURF & yes & 2292 & 962.111 &  \\ \hline 
			ours & SURF & no & 2285 & 36715.3 & \\ \hline 
			\begin{tabular}[l]{@{}c@{}} Visual SFM\\ calibrated\end{tabular} & SIFT & yes & 1835 & -  & 0.02543\\ \hline 
			\begin{tabular}[l]{@{}c@{}} Visual SFM\\ uncalibrated\end{tabular} & SIFT & yes & 1575 & - & 0.02139 \\ \hline 
		\end{tabular}
		\caption{The number of valid structure points, reprojection error and RMSE of the camera pose against reference values on the Model House dataset. The VisualSFM in calibrated case had fixed shared intrinsic camera parameters. The feature are extracted using SIFT extractor and matched by FLANN matcher.}
		\label{tab:precision}
	\end{center}
\end{table}

\begin{figure}[!htbp]
	\begin{center}
		\includegraphics[keepaspectratio,width=\textwidth]{fig/pose_comparison.pdf}
	\end{center}
	\caption{Visualisation of the camera pose estimation measurement.}
	\label{fig:pose_comparison1}
\end{figure} 

The figure \ref{fig:pose_comparison1} shows visual representation of the camera poses in Blender where we have calculated how far off are they compared to the reference values. The only case where the camera poses are far off are the ones of our program without any structure refinement provided by SLAM++. Please note that this measurement is not precise as the scale is ambiguous between different programmes (and maybe even in the same program). Therefore the first and last camera's positions were matched to ensure similar scale and only the gaps between cameras differ. The figure \ref{fig:pose_comparison2} shows how do the optimized (red) and unoptimized (orange) cameras as well as structure differ. It is not quite visible in the picture, but the unoptimized structure is rotated about 15 degrees as well as shifted one tenth of the house length. The optimized structure is however quite on spot with the reference values (black dots).


\begin{figure}[!htbp]
	\begin{center}
		\includegraphics[keepaspectratio,width=\textwidth]{fig/pose_comparison2.pdf}
	\end{center}
	\caption{Visualisation of the camera poses and structure from optimized and unoptimized run on the Model House dataset. The black dots is the reference structure.}
	\label{fig:pose_comparison2}
\end{figure} 

\subsection*{Unordered Case}
While most image collections share same (and often known or easy to estimate) camera calibration, the case where this collection is sequenced and can be represented as single camera track is rather rare. The aim of this section is to evaluate two implemented methods of getting the camera tracks and how do they affect the reconstruction.

TODO:  process ad write the experiments results (2 tables)
\begin{verbatim}
http://imagine.enpc.fr/~moulonp/publis/featureTracking_CVMP12.pdf
http://openmvg.readthedocs.org/en/latest/openMVG/tracks/tracks/
\end{verbatim}
\section{Weakly Calibrated Case}
\label{sec:experiments-weakly}
As of now, the application requires a calibration in order to reconstruct properly. However, in this section we will be briefly evaluating other applications. This will serve as a basis for evaluation of the uncalibrated case once done.

TODO: process and write the values (Červená Lhota dataset)

\chapter{Conclusion and Further Work}
\label{chapter:conclusion}
\textit{This chapter presents the conclusion of the term project work. Further work is also outlined as this topic will be further developed as a part of the Masters thesis.}

\section{Conclusion}
This term project has focused on the study of a means to estimate three-dimensional information from a two-dimensional image sequence. Usually, the first step is to create appropriate dataset. We summarized requirements on such dataset, identified what qualities the images should have and what sort of images should be filtered out. We provided a simple tool that allows downloading images in batches from the Flickr service and explained why we have decided to use additional sources of images as well. 

Another step in the 3D reconstruction is detection of the feature. A number of feature detectors were introduced and their characteristics described. We conducted a series of experiments which goal was to understand qualities of feature detection in context of the building reconstruction. Then the reader was introduced to the issue of extraction of the feature descriptors from the image. The extractors implemented in OpenCV were described and compared one to another. We finished the chapter with feature matching algorithms. All combinations of feature detectors, extractors and matchers were tested on a 100 image pair input set and the result evaluated. We have selected the best combination, FAST detector, SIFT extractor and FLANN matcher, for our problem which maintains a good ratio between performance and efficiency.

The problem of 3D information estimation was discussed and three different approaches of non-contact scanning outlined. We started with the stereo vision, where the depth can be directly computed from the image disparity. The problem gets more difficult when only one camera scans the 3D space. This approach, monocular vision, uses feature to calculate camera position and reconstruct the 3D structure. Lastly we talked about the uncalibrated approach, where the scene is being reconstructed from a number of images made by multiple cameras, each having a different calibration matrix. The pose estimation was outlined and difficulties with such approach identified.

Lastly we have talked about existing solutions that are implementing the uncalibrated approach. Three different programs (Photosynth, VisualSFM and Bundler) were introduced and briefly evaluated. These programs will be matched against the final application implemented within the Masters thesis to evaluate its capabilities and performance.

\section{Further Work}
The research and work presented in this paper proposes the following subjects for further work:
\begin{itemize}
	\item Completion of the feature detection and matching part of the uncalibrated app. As of now, the program can detect and match feature, however, it would be useful to store such information in a binary form to be able to use it as a input for the VisualSFM. This would allow us to test how does the reconstruction improve with a different detector.
	
	\item The feature detection and matching should be further tested in order to find best combination. For now only default parameters were used, which gave an overall idea of the algorithms performance, but with different parameters some of them may perform even better.
	
	\item A further steps should be implemented into the uncalibrated app including, among others, camera calibration and pose estimation and the 3D structure reconstruction. The result of this step will be sparse point cloud that can be visualized using software like Meshlab.
	
	\item With the application being able to create sparse reconstruction, further description of the math behind the problem needs to be provided. This includes but is not limited to fundamental matrix estimation, camera calibration and epipolar geometry.
	
	\item Because Flickr service does not contain enough data from Czech Republic, a script to download pictures from different internet sources like Google Images would be useful. Once images are downloaded, it would be advantageous to filter these data using some application or a custom made program.
\end{itemize}
%=========================================================================
